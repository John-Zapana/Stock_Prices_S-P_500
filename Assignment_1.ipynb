{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4352ae54-f79b-4498-9861-e4afdcbc172e",
   "metadata": {},
   "source": [
    "# 1. Storing and Retrieving Data\n",
    "\n",
    "### Objective:\n",
    "Decide whether to store the data in CSV or Parquet format, considering compression schemes, and justify your choice at scales 1x, 10x, and 100x.\n",
    "\n",
    "## 1.1. Research CSV vs. Parquet\n",
    "- **CSV:** Simple, human-readable, but less efficient in terms of storage and query performance, especially for large datasets.\n",
    "- **Parquet:** Columnar storage format, highly efficient for read-heavy operations, supports compression, and is optimized for big data processing.\n",
    "\n",
    "### Load the CSV Dataset\n",
    "First, let's load the CSV dataset into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03875c55-96e3-45d3-adc3-2fd22bc7ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the all_stocks_5yr.csv file\n",
    "df = pd.read_csv('all_stocks_5yr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a16844-e153-479f-b0c0-0dea116b409f",
   "metadata": {},
   "source": [
    "### Convert CSV to Parquet\n",
    "Next, let's convert the dataset to Parquet format. Parquet is a columnar storage format that is highly efficient for large datasets.\n",
    "- **engine='pyarrow':** PyArrow is a fast and efficient library for working with Parquet files.\n",
    "- **compression='snappy':** Snappy is a popular compression scheme that balances speed and compression ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1f7d83-7293-46f6-b1b8-bfd5365484e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to Parquet format\n",
    "df.to_parquet('all_stocks_5yr.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f7c2b-3361-473f-b3a8-603946f2158f",
   "metadata": {},
   "source": [
    "## 1.2. Benchmarking\n",
    "Now, let's benchmark the performance of CSV and Parquet formats at 1x, 10x, and 100x scales.\n",
    "\n",
    "### Compare Measure Time\n",
    "Measure the time taken to read and write the data.\n",
    "\n",
    "#### 1x Scale (Original Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c672161-7580-413f-9929-928a89fd1c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Load Time: 0.24682259559631348 seconds\n",
      "Parquet Load Time: 0.13376808166503906 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark CSV\n",
    "start_time = time.time()\n",
    "df_csv = pd.read_csv('all_stocks_5yr.csv')\n",
    "csv_load_time = time.time() - start_time\n",
    "\n",
    "# Benchmark Parquet\n",
    "start_time = time.time()\n",
    "df_parquet = pd.read_parquet('all_stocks_5yr.parquet')\n",
    "parquet_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"CSV Load Time: {csv_load_time} seconds\")\n",
    "print(f\"Parquet Load Time: {parquet_load_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931563b7-0d61-4379-9d67-bd287b0b9e15",
   "metadata": {},
   "source": [
    "#### 10x Scale (Simulate 10x Data)\n",
    "To simulate a 10x larger dataset, concatenate the dataset with itself 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09d385e0-b799-443a-9f93-f2cc6aef426e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Load Time (10x): 2.5395877361297607 seconds\n",
      "Parquet Load Time (10x): 0.5917782783508301 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create a 10x larger dataset\n",
    "df_10x = pd.concat([df] * 10, ignore_index=True)\n",
    "\n",
    "# Save the 10x dataset to CSV and Parquet\n",
    "df_10x.to_csv('all_stocks_5yr_10x.csv', index=False)\n",
    "df_10x.to_parquet('all_stocks_5yr_10x.parquet', engine='pyarrow', compression='snappy')\n",
    "\n",
    "# Benchmark CSV\n",
    "start_time = time.time()\n",
    "df_csv_10x = pd.read_csv('all_stocks_5yr_10x.csv')\n",
    "csv_load_time_10x = time.time() - start_time\n",
    "\n",
    "# Benchmark Parquet\n",
    "start_time = time.time()\n",
    "df_parquet_10x = pd.read_parquet('all_stocks_5yr_10x.parquet')\n",
    "parquet_load_time_10x = time.time() - start_time\n",
    "\n",
    "print(f\"CSV Load Time (10x): {csv_load_time_10x} seconds\")\n",
    "print(f\"Parquet Load Time (10x): {parquet_load_time_10x} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2b04a-82fc-4748-b916-9997e0acf198",
   "metadata": {},
   "source": [
    "#### 100x Scale (Simulate 100x Data)\n",
    "Similarly, a 100x larger dataset was simulated by concatenating the data set 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9cbd9c4-d6f7-44a7-87fa-cb943377050e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Load Time (100x): 45.212077617645264 seconds\n",
      "Parquet Load Time (100x): 10.515836954116821 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create a 100x larger dataset\n",
    "df_100x = pd.concat([df] * 100, ignore_index=True)\n",
    "\n",
    "# Save the 100x dataset to CSV and Parquet\n",
    "df_100x.to_csv('all_stocks_5yr_100x.csv', index=False)\n",
    "df_100x.to_parquet('all_stocks_5yr_100x.parquet', engine='pyarrow', compression='snappy')\n",
    "\n",
    "# Benchmark CSV\n",
    "start_time = time.time()\n",
    "df_csv_100x = pd.read_csv('all_stocks_5yr_100x.csv')\n",
    "csv_load_time_100x = time.time() - start_time\n",
    "\n",
    "# Benchmark Parquet\n",
    "start_time = time.time()\n",
    "df_parquet_100x = pd.read_parquet('all_stocks_5yr_100x.parquet')\n",
    "parquet_load_time_100x = time.time() - start_time\n",
    "\n",
    "print(f\"CSV Load Time (100x): {csv_load_time_100x} seconds\")\n",
    "print(f\"Parquet Load Time (100x): {parquet_load_time_100x} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a88e577-22f6-408d-8a59-f6dd45b19261",
   "metadata": {},
   "source": [
    "### Compare Storage Size\n",
    "Compare the file sizes of CSV and Parquet files at each scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d460c3e4-4f44-4e69-9527-ee0fc055cbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Size (1x): 28.704509735107422 MB\n",
      "Parquet Size (1x): 10.14011001586914 MB\n",
      "\n",
      "CSV Size (10x): 288.005407333374 MB\n",
      "Parquet Size (10x): 95.29972076416016 MB\n",
      "\n",
      "CSV Size (100x): 2880.053747177124 MB\n",
      "Parquet Size (100x): 951.1515684127808 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get file sizes\n",
    "csv_size = os.path.getsize('all_stocks_5yr.csv') / (1024 * 1024)  # Size in MB\n",
    "parquet_size = os.path.getsize('all_stocks_5yr.parquet') / (1024 * 1024)  # Size in MB\n",
    "\n",
    "csv_size_10x = os.path.getsize('all_stocks_5yr_10x.csv') / (1024 * 1024)\n",
    "parquet_size_10x = os.path.getsize('all_stocks_5yr_10x.parquet') / (1024 * 1024)\n",
    "\n",
    "csv_size_100x = os.path.getsize('all_stocks_5yr_100x.csv') / (1024 * 1024)\n",
    "parquet_size_100x = os.path.getsize('all_stocks_5yr_100x.parquet') / (1024 * 1024)\n",
    "\n",
    "print(f\"CSV Size (1x): {csv_size} MB\")\n",
    "print(f\"Parquet Size (1x): {parquet_size} MB\\n\")\n",
    "print(f\"CSV Size (10x): {csv_size_10x} MB\")\n",
    "print(f\"Parquet Size (10x): {parquet_size_10x} MB\\n\")\n",
    "print(f\"CSV Size (100x): {csv_size_100x} MB\")\n",
    "print(f\"Parquet Size (100x): {parquet_size_100x} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2801b2-9f8d-426e-85ca-1398e6a96ff1",
   "metadata": {},
   "source": [
    "## 1.3. Analysis of Results\n",
    "\n",
    "### Load Times\n",
    "- **1x Scale:** Parquet is **faster** than CSV.\n",
    "- **10x Scale:** Parquet is **faster** than CSV.\n",
    "- **100x Scale:** Parquet is **faster** than CSV.\n",
    "\n",
    "**Key Insight:** As the dataset size increases, Parquet consistently outperforms CSV in terms of load times. The performance gap becomes more significant at larger scales (10x and 100x).\n",
    "\n",
    "### File Sizes\n",
    "- **1x Scale:** Parquet is **smaller** than CSV.\n",
    "- **10x Scale:** Parquet is **smaller** than CSV.\n",
    "- **100x Scale:** Parquet is **smaller** than CSV.\n",
    "\n",
    "**Key Insight:** Parquet files are significantly smaller than CSV files at all scales. This is due to Parquet's columnar storage format and efficient compression (e.g., Snappy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2d77a-90ad-4b5e-9ec9-3f8665d60ad9",
   "metadata": {},
   "source": [
    "## 1.4. Recommendation\n",
    "Based on the benchmarking results, here’s the recommendation:\n",
    "\n",
    "- **For small datasets (1x):** Parquet is recommended for its speed and efficiency, but CSV is acceptable if simplicity is a priority.\n",
    "- **For medium to large datasets (10x and 100x):** Parquet is the clear winner due to its superior performance in both load times and storage efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f68418-a3f8-4dcc-8ffd-d2f5b4b7b4a9",
   "metadata": {},
   "source": [
    "# 2. Manipulating, Analyzing Data, and Building Models\n",
    "\n",
    "### Objective:\n",
    "Compare the performance of Pandas and Polars for data manipulation and analysis, and build prediction models using technical indicators.\n",
    "\n",
    "### Dataset Details\n",
    "The dataset has 7 columns with 619,040 rows. Here's a breakdown of its key features:\n",
    "- **Target Variable:** close (closing price of a company's stock).\n",
    "- **Numerical Features:** open, high, low, and volume.\n",
    "- **Categorical Features:** date, and name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d4792e-4c1b-4081-9dac-2afeb25223d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 619040 entries, 0 to 619039\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   date    619040 non-null  object \n",
      " 1   open    619029 non-null  float64\n",
      " 2   high    619032 non-null  float64\n",
      " 3   low     619032 non-null  float64\n",
      " 4   close   619040 non-null  float64\n",
      " 5   volume  619040 non-null  int64  \n",
      " 6   name    619040 non-null  object \n",
      "dtypes: float64(4), int64(1), object(2)\n",
      "memory usage: 33.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "df_parquet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94223f5d-510b-4a71-ac23-19503f6ebba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       0\n",
      "open      11\n",
      "high       8\n",
      "low        8\n",
      "close      0\n",
      "volume     0\n",
      "name       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the missing values\n",
    "print(df_parquet.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aecaff-a5dc-4ffd-8552-2e4a4098a5c8",
   "metadata": {},
   "source": [
    "### Handle Missing Values\n",
    "Handling missing values is an important step before proceeding with any analysis or modelling.\n",
    "\n",
    "Based on the Dataset Details output, it seems there are:\n",
    "- 11 missing values in **open**\n",
    "- 8 missing values in **high**\n",
    "- 8 missing values in **low**\n",
    "\n",
    "So because the number of missing values is small, we can **drop** those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6499d0b-89aa-41e0-854c-08821e7bebcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date      0\n",
      "open      0\n",
      "high      0\n",
      "low       0\n",
      "close     0\n",
      "volume    0\n",
      "name      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "df_cleaned = df_parquet.dropna().copy()\n",
    "\n",
    "# Verify that there are no missing values\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51a76f-bd3e-4bd0-b372-50ae8e0b8371",
   "metadata": {},
   "source": [
    "## 2.1. Compare Pandas vs. Polars\n",
    "We’ll compare the performance of Pandas and Polars for data manipulation tasks. As we will see later, **Polars is a faster** alternative to Pandas, especially for large datasets.\n",
    "- Perform common data manipulation tasks (e.g., filtering, grouping, aggregating) using both Pandas and Polars.\n",
    "- Measure the time taken for each operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88e0e8a6-e04a-4ec6-b5f7-a437b2ad6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "\n",
    "# Convert the cleaned Pandas DataFrame to a Polars DataFrame\n",
    "df_polars = pl.from_pandas(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24223681-4676-489e-b07f-42cb31ba184f",
   "metadata": {},
   "source": [
    "### Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ec40c7-6fc8-4903-9651-a4c874c4142b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Filtering Time: 0.016785144805908203 seconds\n",
      "Polars Filtering Time: 0.048210859298706055 seconds\n"
     ]
    }
   ],
   "source": [
    "# Benchmark Pandas: Filtering data\n",
    "start_time = time.time()\n",
    "df_filtered_pandas = df_cleaned[df_cleaned['volume'] > 1000000]\n",
    "pandas_filter_time = time.time() - start_time\n",
    "\n",
    "# Benchmark Polars: Filtering data\n",
    "start_time = time.time()\n",
    "df_filtered_polars = df_polars.filter(pl.col('volume') > 1000000)\n",
    "polars_filter_time = time.time() - start_time\n",
    "\n",
    "# Measure the time taken for this operation\n",
    "print(f\"Pandas Filtering Time: {pandas_filter_time} seconds\")\n",
    "print(f\"Polars Filtering Time: {polars_filter_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd9db7-bc93-465b-8ea3-17a9fcc9f214",
   "metadata": {},
   "source": [
    "### Grouping and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78af442e-60c0-4381-820c-42cc5224b89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Grouping Time: 0.02505803108215332 seconds\n",
      "Polars Grouping Time: 0.033209800720214844 seconds\n"
     ]
    }
   ],
   "source": [
    "# Benchmark Pandas: Grouping and Aggregating\n",
    "start_time = time.time()\n",
    "df_grouped_pandas = df_cleaned.groupby('name')['volume'].mean()\n",
    "pandas_group_time = time.time() - start_time\n",
    "\n",
    "# Benchmark Polars: Grouping and Aggregating\n",
    "start_time = time.time()\n",
    "df_grouped_polars = df_polars.group_by('name').agg(pl.col('volume').mean())\n",
    "polars_group_time = time.time() - start_time\n",
    "\n",
    "# Measure the time taken for this operation\n",
    "print(f\"Pandas Grouping Time: {pandas_group_time} seconds\")\n",
    "print(f\"Polars Grouping Time: {polars_group_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37e056-5d8c-4e3d-85da-2450e79e30a0",
   "metadata": {},
   "source": [
    "## 2.2. Technical Indicators\n",
    "Technical indicators are essential for analyzing stock prices. Now that the data is clean, we will calculate 4 technical indicators such as Moving Average (MA), Relative Strength Index (RSI), Moving Average Converge Divergence (MACD), Bollinger Bands; and we will add them as new columns to the dataframe.\n",
    "\n",
    "Let’s calculate these indicators for each stock (grouped by **name**).\n",
    "\n",
    "### Moving Average (MA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fe5a838-194d-4555-9780-e48e983bae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by stock name\n",
    "grouped_no_name = df_polars.drop('name').group_by('name', maintain_order=True)\n",
    "\n",
    "# Function to calculate Moving Average (MA)\n",
    "def calculate_ma(window=14):\n",
    "    return pl.col('close').rolling_mean(window).alias('MA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff33688-1118-4848-ab8d-94a04a4e9fb3",
   "metadata": {},
   "source": [
    "### Relative Strength Index (RSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f38a6a49-ccf3-4a95-9ace-13cd85e99b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Relative Strength Index (RSI)\n",
    "def calculate_rsi(window=14):\n",
    "    delta = pl.col('close').diff().fill_null(0)\n",
    "    gain = pl.when(delta > 0).then(delta).otherwise(0).rolling_mean(window).alias('RSI_Gain')\n",
    "    loss = pl.when(delta < 0).then(-delta).otherwise(0).rolling_mean(window).alias('RSI_Loss')\n",
    "    rs = gain / loss\n",
    "    rsi = (100 - (100 / (1 + rs))).alias('RSI')\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb5eea5-b4a6-45d0-bee6-2476c3ae3f1a",
   "metadata": {},
   "source": [
    "### Moving Average Convergence Divergence (MACD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41009198-ba38-4808-9282-e3fa4dc5e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate MACD\n",
    "def calculate_macd(short_window=12, long_window=26, signal_window=9):\n",
    "    short_ema = pl.col('close').ewm_mean(span=short_window, adjust=False).alias('MACD')\n",
    "    long_ema = pl.col('close').ewm_mean(span=long_window, adjust=False).alias('MACD')\n",
    "    macd = (short_ema - long_ema).alias('MACD')\n",
    "    signal = macd.ewm_mean(span=signal_window, adjust=False).alias('MACD_Signal')\n",
    "    return macd, signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ecf41-e84e-4bf0-95ba-343faf096000",
   "metadata": {},
   "source": [
    "### Bollinger Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e0b98f9-9b43-47a0-990b-e7e6424e1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bollinger_bands(window=20, num_std=2):\n",
    "    rolling_mean = pl.col('close').rolling_mean(window).alias('Bollinger_Mid')\n",
    "    rolling_std = pl.col('close').rolling_std(window)\n",
    "    upper_band = (rolling_mean + (rolling_std * num_std)).alias('Bollinger_Upper')\n",
    "    lower_band = (rolling_mean - (rolling_std * num_std)).alias('Bollinger_Lower')\n",
    "    return rolling_mean, upper_band, lower_band"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e744728b-8fbb-4456-8655-215b9204b2e3",
   "metadata": {},
   "source": [
    "#### Add Indicators to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d59934e-e391-4432-9708-09150eb29d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 14)\n",
      "┌────────────┬───────┬───────┬───────┬───┬─────────────┬──────────────┬──────────────┬─────────────┐\n",
      "│ date       ┆ open  ┆ high  ┆ low   ┆ … ┆ MACD_Signal ┆ Bollinger_Mi ┆ Bollinger_Up ┆ Bollinger_L │\n",
      "│ ---        ┆ ---   ┆ ---   ┆ ---   ┆   ┆ ---         ┆ d            ┆ per          ┆ ower        │\n",
      "│ str        ┆ f64   ┆ f64   ┆ f64   ┆   ┆ f64         ┆ ---          ┆ ---          ┆ ---         │\n",
      "│            ┆       ┆       ┆       ┆   ┆             ┆ f64          ┆ f64          ┆ f64         │\n",
      "╞════════════╪═══════╪═══════╪═══════╪═══╪═════════════╪══════════════╪══════════════╪═════════════╡\n",
      "│ 2013-02-08 ┆ 15.07 ┆ 15.12 ┆ 14.63 ┆ … ┆ 0.0         ┆ null         ┆ null         ┆ null        │\n",
      "│ 2013-02-11 ┆ 14.89 ┆ 15.01 ┆ 14.26 ┆ … ┆ -0.004627   ┆ null         ┆ null         ┆ null        │\n",
      "│ 2013-02-12 ┆ 14.45 ┆ 14.51 ┆ 14.1  ┆ … ┆ -0.014932   ┆ null         ┆ null         ┆ null        │\n",
      "│ 2013-02-13 ┆ 14.3  ┆ 14.94 ┆ 14.25 ┆ … ┆ -0.021999   ┆ null         ┆ null         ┆ null        │\n",
      "│ 2013-02-14 ┆ 14.94 ┆ 14.96 ┆ 13.16 ┆ … ┆ -0.037307   ┆ null         ┆ null         ┆ null        │\n",
      "└────────────┴───────┴───────┴───────┴───┴─────────────┴──────────────┴──────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Apply the functions to each group\n",
    "df_polars = df_polars.with_columns([\n",
    "    calculate_ma().over('name'),\n",
    "    calculate_rsi().over('name'),\n",
    "    calculate_macd()[0].over('name'),  # Apply MACD calculation to each group\n",
    "    calculate_macd()[1].over('name'),\n",
    "    calculate_bollinger_bands()[0].over('name'),  # Apply Bollinger Bands calculation to each group\n",
    "    calculate_bollinger_bands()[1].over('name'),\n",
    "    calculate_bollinger_bands()[2].over('name')\n",
    "])\n",
    "\n",
    "# Display the first few rows with the new columns\n",
    "print(df_polars.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf412a4-6a54-4219-9de2-7eee5c2fdb69",
   "metadata": {},
   "source": [
    "## 2.3 Build Prediction Models\n",
    "Now, let’s build prediction models (Linear Regression, Random Forest) to forecast the next day's closing price. We’ll use an 80-20 train-test split and evaluate the models using **RMSE (Root Mean Squared Error)**.\n",
    "\n",
    "#### Handling missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b47fba63-8d92-4b08-91b4-0c3abcf1e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': shape: (1,)\n",
      "Series: 'date' [u32]\n",
      "[\n",
      "\t0\n",
      "], 'open': shape: (1,)\n",
      "Series: 'open' [u32]\n",
      "[\n",
      "\t0\n",
      "], 'high': shape: (1,)\n",
      "Series: 'high' [u32]\n",
      "[\n",
      "\t0\n",
      "], 'low': shape: (1,)\n",
      "Series: 'low' [u32]\n",
      "[\n",
      "\t0\n",
      "], 'close': shape: (1,)\n",
      "Series: 'close' [u32]\n",
      "[\n",
      "\t0\n",
      "], 'volume': shape: (1,)\n",
      "Series: 'volume' [u32]\n",
      "[\n",
      "\t0\n",
      "], 'name': shape: (1,)\n",
      "Series: 'name' [u32]\n",
      "[\n",
      "\t0\n",
      "], 'MA': shape: (1,)\n",
      "Series: 'MA' [u32]\n",
      "[\n",
      "\t6565\n",
      "], 'RSI': shape: (1,)\n",
      "Series: 'RSI' [u32]\n",
      "[\n",
      "\t6565\n",
      "], 'MACD': shape: (1,)\n",
      "Series: 'MACD' [u32]\n",
      "[\n",
      "\t0\n",
      "], 'MACD_Signal': shape: (1,)\n",
      "Series: 'MACD_Signal' [u32]\n",
      "[\n",
      "\t0\n",
      "], 'Bollinger_Mid': shape: (1,)\n",
      "Series: 'Bollinger_Mid' [u32]\n",
      "[\n",
      "\t9595\n",
      "], 'Bollinger_Upper': shape: (1,)\n",
      "Series: 'Bollinger_Upper' [u32]\n",
      "[\n",
      "\t9595\n",
      "], 'Bollinger_Lower': shape: (1,)\n",
      "Series: 'Bollinger_Lower' [u32]\n",
      "[\n",
      "\t9595\n",
      "]}\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the df_cleaned data after adding indicators\n",
    "print(df_polars.null_count().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08639297-8491-49b6-99ab-f60738050797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the mean of each column\n",
    "df_polars = df_polars.fill_null(strategy='mean')\n",
    "\n",
    "# Verify no missing values remain\n",
    "#print(df_polars.null_count().to_dict())\n",
    "\n",
    "# Convert Polars DataFrame to Pandas for model training\n",
    "df_cleaned = df_polars.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e0810f-f889-4002-8fc4-821a1ba93b77",
   "metadata": {},
   "source": [
    "### Models: Linear Regression & Random Forest\n",
    "We’ll use the technical indicators (**MA**, **RSI**, **MACD**, **MACD_Signal**, **Bollinger_Mid**, **Bollinger_Upper**, **Bollinger_Lower**) as features and the **close** price as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7129d6a-0393-4f9a-b134-0f1edada20fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 0.8267125459108222\n",
      "Random Forest RMSE: 0.8549893251085081\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Features and target\n",
    "features = ['high', 'low', 'MA', 'RSI'] # the features more relvant are low and high (plot features importance)\n",
    "X = df_cleaned[features]\n",
    "y = df_cleaned['close']\n",
    "\n",
    "# Split the data into training and testing sets (80-20 train test-split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1) Linear Regression\n",
    "# Train the model\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)  # Calculate mean squared error\n",
    "rmse_lr = mse_lr ** 0.5  # Calculate root mean squared error\n",
    "print(f\"Linear Regression RMSE: {rmse_lr}\")\n",
    "\n",
    "# 2) Random Forest\n",
    "# Train the model\n",
    "model_rf = RandomForestRegressor(n_estimators=100, max_depth=30, min_samples_split=4, min_samples_leaf=2, random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = mse_rf ** 0.5  # Calculate root mean squared error\n",
    "print(f\"Random Forest RMSE: {rmse_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a944d4d4",
   "metadata": {},
   "source": [
    "#### Analysis of Models\n",
    "- **Linear Regression:**\n",
    "    - The RMSE of **0.826** indicates that, on average, the model's predictions are off by approximately 0.834 units from the actual closing prices.\n",
    "    - This is a reasonable performance for a simple model like Linear Regression.\n",
    "\n",
    "- **Random Forest:**\n",
    "    - The RMSE of **0.854** is slightly higher than that of Linear Regression, which is unexpected because Random Forest is typically more powerful and flexible.\n",
    "    - This could be due to **Underfitting**: The model may not have enough trees (**n_estimators=100**) to capture the complexity of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6aad2-1256-49d6-b1bb-e6ae91309012",
   "metadata": {},
   "source": [
    "# 3. Creating a Visual Dashboard\n",
    "### Objective:\n",
    "Research and compare dashboarding libraries (e.g., Streamlit, Dash, Reflex) and create a dashboard to display benchmark results and price predictions.\n",
    "\n",
    "### Saving Predictions and Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61edf60a-2f6b-4075-9c7e-d35ab81b4748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stock predictions saved to stock_predictions.csv\n",
      "✅ Benchmark results saved to benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Save stock predictions to CSV\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"date\": df_cleaned.iloc[y_test.index][\"date\"].values,\n",
    "    \"ticker\": df_cleaned.iloc[y_test.index][\"name\"].values,\n",
    "    \"actual_price\": y_test.values,\n",
    "    \"predicted_price\": y_pred_rf  # Using Random Forest predictions\n",
    "})\n",
    "predictions_df.to_csv(\"stock_predictions.csv\", index=False)\n",
    "print(\"✅ Stock predictions saved to stock_predictions.csv\")\n",
    "\n",
    "# Save benchmark results to CSV\n",
    "benchmark_results = pd.DataFrame({\n",
    "    \"Dataset_Size\": [\"1x\", \"10x\", \"100x\"],\n",
    "    \"CSV_Load_Time\": [csv_load_time, csv_load_time_10x, csv_load_time_100x],\n",
    "    \"Parquet_Load_Time\": [parquet_load_time, parquet_load_time_10x, parquet_load_time_100x],\n",
    "    \"CSV_Size_MB\": [csv_size, csv_size_10x, csv_size_100x],\n",
    "    \"Parquet_Size_MB\": [parquet_size, parquet_size_10x, parquet_size_100x],\n",
    "    \"Pandas_Filter_Time\": [pandas_filter_time, None, None],  # Only measured for 1x\n",
    "    \"Polars_Filter_Time\": [polars_filter_time, None, None],\n",
    "    \"Pandas_Group_Time\": [pandas_group_time, None, None],\n",
    "    \"Polars_Group_Time\": [polars_group_time, None, None]\n",
    "})\n",
    "benchmark_results.to_csv(\"benchmark_results.csv\", index=False)\n",
    "print(\"✅ Benchmark results saved to benchmark_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73eb9a-7818-4fce-ba68-3d0a50e72f28",
   "metadata": {},
   "source": [
    "## Dashboards A) & B)\n",
    "- To disaply tha dashboard we must open anaconda promt and then run this:\n",
    "    - **streamlit run dashboard.py**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.2",
   "language": "python",
   "name": "env3_13_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
